% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{xr}
\externaldocument{LMAms_main}
\usepackage{float}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{fancyhdr}
\pagestyle{fancy}
\usepackage[default]{sourcesanspro}
\usepackage{sourcecodepro}
\usepackage{fvextra}
\usepackage{multirow}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
\fancyhead[RE,RO]{Katabuchi \textit{et al}.  Appendix}
\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Katabuchi et al., Decomposing leaf mass into metabolic and structural components explains divergent patterns of trait variation within and among plant species},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Katabuchi et al., Decomposing leaf mass into metabolic and
structural components explains divergent patterns of trait variation
within and among plant species}
\author{}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[sharp corners, borderline west={3pt}{0pt}{shadecolor}, frame hidden, enhanced, breakable, boxrule=0pt, interior hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\newpage

\hypertarget{appendix-s1-prior-information}{%
\section{Appendix S1: Prior
information}\label{appendix-s1-prior-information}}

The logarithms of \emph{A}\textsubscript{area},
\emph{R}\textsubscript{area}, and LL for leaf sample \emph{i} were
assumed to have a multivariate normal distribution:

\[
\begin{aligned}
\left(
\begin{array}{ccc}
\mathrm{ln}(A_{\mathrm{area} \, i})\\
\mathrm{ln}(R_{\mathrm{area} \, i}) \\
\mathrm{ln}(\mathrm{LL}_i)
\end{array}
\right)
\sim \mathrm{MVN}
\left(
\begin{array}{rrr}
\mathrm{E}[A_{\mathrm{area} \, i}] & \\
\mathrm{E}[R_{\mathrm{area} \, i}] &, \boldsymbol{\Sigma}\\
\mathrm{E}[\mathrm{LL}_i] &
\end{array}
\right) \qquad(\mathrm{S}1)
\end{aligned}
\]

where E{[}\(\cdot\){]} indicates expected value and
\(\boldsymbol{\Sigma}\) indicates a covariance matrix. Expected values
are based on Eqs. 2- 4 in the main text.

We used non-informative or weakly informative prior distributions
(\protect\hyperlink{ref-Lemoine2019}{Lemoine, 2019}). The covariance
matrix in Eq. S1 was decomposed as
\({\boldsymbol \Sigma} = \mathrm {diag}(\boldsymbol {\sigma}){\boldsymbol \Omega}\mathrm {diag}({\boldsymbol \sigma}) = \mathrm {diag}({\boldsymbol \sigma}){\boldsymbol L}{\boldsymbol L}\prime \mathrm {diag}({\boldsymbol \sigma})\)
using a Cholesky decomposition, where \(\boldsymbol {\sigma}\) is a
vector of \(\sigma_{1}\), \(\sigma_{2}\), and \(\sigma_{3}\);
\({\boldsymbol \Omega}\) is a correlation matrix of \(\rho_{12}\),
\(\rho_{13}\), and \(\rho_{23}\); and \textbf{L} is a lower triangular
matrix. Instead of assigning prior distributions on
\(\boldsymbol{\Sigma}\) directly, priors were assigned on
\({\boldsymbol \sigma}\) and \textbf{L} to avoid a strong dependence
between \({\boldsymbol \sigma}\) and \({\boldsymbol \Omega}\)
(\protect\hyperlink{ref-Alvarez2014}{Alvarez et al., 2014};
\protect\hyperlink{ref-Lewandowski2009}{Lewandowski et al., 2009}). A
prior for \textbf{L} was specified as a so-called LKJ distribution with
shape parameter 2 (\protect\hyperlink{ref-Lewandowski2009}{Lewandowski
et al., 2009}), which is weakly informative for the correlation matrix.
A prior for \(\boldsymbol{\sigma}\) was specified as a Half-Cauchy
distribution with location 0 and scale 2.5, which is weakly informative
and allows for occasional large coefficients while still performing a
reasonable amount of shrinkage for coefficients near zero
(\protect\hyperlink{ref-Gelman2008}{Gelman et al., 2008}). Priors for
\(\alpha_{0,m,s}\), \(\beta_{0,m,s}\), and \(\gamma_{0,m,s}\) in Eqs. 2-
4 were weakly informative and specified as normal distributions with
mean 0 and standard deviation 5. Priors for \emph{f\textsubscript{i}} in
Eqs. 1- 4 were non-informative and specified as uniform distributions
with range (0, 1).

\newpage

\hypertarget{appendix-s2-model-constraints}{%
\section{Appendix S2: Model
constraints}\label{appendix-s2-model-constraints}}

To ensure that the model was identifiable, we imposed two broad
assumptions: (i) \emph{A}\textsubscript{area} depends more strongly on
metabolic leaf mass (LMAm: parameter \(\alpha_m\) in Eq. 2) than
structural leaf mass (LMAs: parameter \(\alpha_s\)), and (ii) LL depends
more strongly on LMAs (\(\beta_s\) in Eq. 4 than LMAm (\(\beta_m\)). The
first assumption was implemented in different model versions either by
setting \(\alpha_s\) = 0 or by imposing the constraint \(\alpha_m\)
\textgreater{} \(\alpha_s\). Similarly, the second assumption was
implemented either by setting \(\beta_m\) = 0 or by imposing the
constraint \(\beta_s\) \textgreater{} \(\beta_m\). The weaker form of
these assumptions (\(\alpha_m\) \textgreater{} \(\alpha_s\) and
\(\beta_s\) \textgreater{} \(\beta_m\)) is primarily a labeling
convention and only weakly constrains the possible biological model
outcomes by excluding the possibility that a single LMA component could
be the primary determinant of both \emph{A}\textsubscript{area} and LL.
The stronger form of the assumptions (\(\alpha_s\) = 0 and \(\beta_m\) =
0) leads to a more parsimonious model (fewer parameters). We considered
different combinations of the strong and weak forms of the assumptions
for \emph{A}\textsubscript{area} (\(\alpha_m\) and \(\alpha_s\)) and LL
(\(\beta_m\) and \(\beta_s\)) using cross-validation. We did not impose
any constraints on \emph{R}\textsubscript{area} (\(\gamma_m\) and
\(\gamma_s\)).

\newpage

\hypertarget{appendix-s3-model-tests-with-randomized-data-including-table-as1-and-figure-as1}{%
\section{Appendix S3: Model tests with randomized data (including Table
AS1 and Figure
AS1)}\label{appendix-s3-model-tests-with-randomized-data-including-table-as1-and-figure-as1}}

Because our two-dimensional (LMAm-LMAs) modeling approach includes many
parameters (one latent variable \emph{f\textsubscript{i}} to partition
LMA into LMAm and LMAs for each leaf sample), we implemented tests with
randomized data to assess potential overfitting. We generated 10
different randomized datasets by randomizing all trait values (LMA,
\emph{A}\textsubscript{area}, \emph{R}\textsubscript{area} and LL)
across species. Thus, the randomized datasets had zero expected
covariance among traits. Models fit to the randomized datasets either
did not converge or showed divergent transitions (Appendix S3),
indicating that models fit to randomized data do not provide reliable
inferences (\protect\hyperlink{ref-Betancourt2016}{Betancourt, 2016}).
Furthermore, when models were fit to randomized data, the scaling
exponents (\(\alpha_{m,s}\), \(\beta_{m,s}\), and \(\gamma_{m,s}\)) were
not significantly different from zero. In simple terms, the
two-dimensional models failed when fit to randomized data. In contrast,
when fit to the observed (non-randomized) data, the two-dimensional
models converged without divergent transitions and out-performed
one-dimensional models (total LMA) for both GLOPNET and Panama (see
Results). Thus, the tests with randomized data indicate that our
two-dimensional approach is not inherently prone to overfitting or to
creating spurious results. We therefore assume that estimates of LMAm
and LMAs obtained from the GLOPNET and Panama datasets reflect
meaningful patterns in the observations and allow for a meaningful
exploration of our questions.

We generated 10 randomized datasets for each of the GLOPNET and the
Panama datasets by randomly shuffling each trait value across leaf
samples, and we fit the best models (Table 1) to the randomized data.

Most of the model results obtained from the randomized datasets did not
convergent based on the Gelman-Rubin statistic or showed divergent
transitions (Table AS1), indicating that models fit to randomized data
do not provide reliable inferences
(\protect\hyperlink{ref-Betancourt2016}{Betancourt, 2016}).

\hypertarget{table-as1}{%
\subsection{Table AS1}\label{table-as1}}

\begin{itemize}
\item
  No\_large\_Rhat: The number of parameters (including transformed
  parameters) that shows Rhat (Gelman-Rubin statistic) greater than 1.1.
\item
  No\_divergence: The number of iterations that shows divergent
  transitions.
\end{itemize}

\begin{longtable}[]{@{}llrr@{}}
\toprule()
Data & Simulation\_ID & No\_large\_Rhat & No\_divergence \\
\midrule()
\endhead
GLOPNET & sim-01 & 4 & 13 \\
GLOPNET & sim-02 & 4 & 0 \\
GLOPNET & sim-03 & 0 & 0 \\
GLOPNET & sim-04 & 0 & 0 \\
GLOPNET & sim-05 & 0 & 1 \\
GLOPNET & sim-06 & 0 & 1 \\
GLOPNET & sim-07 & 12 & 0 \\
GLOPNET & sim-08 & 4 & 0 \\
GLOPNET & sim-09 & 0 & 0 \\
GLOPNET & sim-10 & 104 & 1 \\
Panama & sim-01 & 4 & 131 \\
Panama & sim-02 & 64 & 23 \\
Panama & sim-03 & 397 & 28 \\
Panama & sim-04 & 0 & 15 \\
Panama & sim-05 & 7 & 58 \\
Panama & sim-06 & 79 & 4 \\
Panama & sim-07 & 7 & 122 \\
Panama & sim-08 & 160 & 131 \\
Panama & sim-09 & 451 & 579 \\
Panama & sim-10 & 601 & 148 \\
\bottomrule()
\end{longtable}

\newpage

\hypertarget{figure-as1}{%
\subsection{Figure AS1}\label{figure-as1}}

Although many parameters in models fit to randomized data had large Rhat
values (Table AS1), which suggests that the posterior distributions did
not converge, we nevertheless examined the regression coefficients to
evaluate if our model framework is prone to overfitting. Figure AS1
shows the regression coefficients for the randomized GLOPNET data
(\(\alpha_{0, m, s}\), \(\beta_{0, s}\), and \(\gamma_{0, m, s}\) in
Eqs. 2- 4 in the main text). There are 10 independent simulations
(randomizations) in total. Points and lines indicate posterior medians
and 95\% credible intervals (CIs), respectively. Although intercepts
were significant, the scaling parameters (\(\alpha_{m, s}\),
\(\beta_{s}\), and \(\gamma_{m, s}\)) obtained for the randomized
datasets were not significantly different from zero. These tests with
randomized data indicate that our model is not inherently prone to
overfitting or to producing patterns from noise.

\includegraphics{/home/mattocci/LMAms/figs/coef_sim_gl.png}

\newpage

\hypertarget{figure-as2}{%
\subsection{Figure AS2}\label{figure-as2}}

Figure AS2 showsWe also checked the regression coefficients for the
randomized Panama dataset (\(\alpha_{0, m}\), \(\beta_{0, s}\),
\(\gamma_{0, m, s}\), and \(\theta\) in Eqs. 2- 4 in the main text).
Details as for Figure AS1. The scaling parameters (\(\alpha_{m}\),
\(\beta_{s}\), and \(\gamma_{m, s}\)) and the effect of light on leaf
lifespan (\(\theta\)) were not significantly different from zero in the
randomized datasets did not show any patterns. Again, these tests with
randomized data indicate that our model is not inherently prone to
overfitting or to producing patterns from noise.

\includegraphics{/home/mattocci/LMAms/figs/coef_sim_pa.png}

\newpage

\hypertarget{appendix-s4-understanding-relationships-between-photosynthetic-capacity-and-lma}{%
\section{Appendix S4: Understanding relationships between photosynthetic
capacity and
LMA}\label{appendix-s4-understanding-relationships-between-photosynthetic-capacity-and-lma}}

We applied our LMAm-LMAs model to simulated data to better understand
relationships between photosynthetic capacity
(\emph{A}\textsubscript{max}) and LMA. The causes and interpretation of
relationships between \emph{A}\textsubscript{max} and LMA are
controversial (\protect\hyperlink{ref-Westoby2013}{Westoby et al.,
2013}). Although \emph{A}\textsubscript{max} is often mass-normalized
(\protect\hyperlink{ref-Blonder2011}{Blonder et al., 2011};
\protect\hyperlink{ref-Shipley2006}{Shipley et al., 2006}; e.g.,
\protect\hyperlink{ref-Wright2004a}{Wright et al., 2004}), it has been
argued that \emph{A}\textsubscript{max} should be area-normalized when
exploring trait relationships, because photosynthesis is an area-based
process (\protect\hyperlink{ref-Lloyd2013}{Lloyd et al., 2013}).
Consistent with this argument, Osnas et al.
(\protect\hyperlink{ref-Osnas2013}{2013}) showed that across global
species, variation in whole-leaf \emph{A}\textsubscript{max} is strongly
dependent on leaf area, but only weakly dependent on leaf mass (after
controlling for variation in leaf area). Osnas et al.
(\protect\hyperlink{ref-Osnas2018}{2018}) further showed that the
relationship between \emph{A}\textsubscript{max} and LMA (i.e., the
degree of mass- vs.~area-dependence) is sensitive to the amount of LL
variation in an assemblage, which we hypothesize depends on the fraction
of total LMA variance in the assemblage that is due to LMAs variance.

To better understand the factors affecting relationships between
\emph{A}\textsubscript{max} and LMA, we created simulated datasets in
which we varied the following factors: the sensitivity of
\emph{A}\textsubscript{area} to variation in LMAm (parameter
\(\alpha_m\) in Eq. 2), the sensitivity of \emph{A}\textsubscript{area}
to LMAs (parameter \(\alpha_s\) in Eq. 2), and the fraction of total LMA
variance due to variance in LMAs. For each simulated dataset, we
quantified the \emph{A}\textsubscript{max} vs.~LMA relationship
following Osnas et al. (\protect\hyperlink{ref-Osnas2018}{2018}):

\[
A_{\mathrm{area} \, i} = c (LMA_i)^{b}\epsilon_i \qquad(\mathrm{S}4)
\]

where LMA is the sum of LMAm and LMAs Eq. 1, \emph{c} is a fitted
constant, and \emph{b} is an index of mass-dependence as illustrated by
the following cases (\protect\hyperlink{ref-Osnas2018}{Osnas et al.,
2018}, \protect\hyperlink{ref-Osnas2013}{2013}): if \emph{b} = 0, then
\emph{A}\textsubscript{area} is independent of LMA, which implies that
whole-leaf \emph{A}\textsubscript{max} is proportional to leaf area;
conversely, if \emph{b} = 1, then \emph{A}\textsubscript{area} is
proportional to LMA, which implies that whole-leaf
\emph{A}\textsubscript{max} is proportional to leaf mass. Intermediate
cases (0 \textless{} \emph{b} \textless{} 1), as well as more extreme
cases (\emph{b} \(\le\) 0 or \emph{b} \(\geq\) 1), are also possible,
with reported estiamtes being between 0 and 1
(\protect\hyperlink{ref-Osnas2018}{Osnas et al., 2018}). Note that
applying Eq. S4 to either mass- or area-normalized
\emph{A}\textsubscript{max} yields equivalent results
(\protect\hyperlink{ref-Osnas2018}{Osnas et al., 2018}).

For these simulations, we generated LMAm and LMAs values based on their
estimated distributions in our GLOPNET and Panama analyses, while
varying the LMAs variance (so that LMAs contributed different fractions
of total LMA variance in different simulated datasets). Although Panama
sun and shade leaves were analyzed together when fitting the LMAm-LMAs
models Eqs. 2 - 4, they were analyzed separately here due to their
different estimated covariance structures (see the main text).

For GLOPNET and Panama sun leaves, LMAm and LMAs were generated
independently of each other, consistent with the low estimated
correlations between LMAm and LMAs in these assemblages (see Results).
Specifically, LMAm values were randomly sampled from a lognormal
distribution with the log-scale mean and standard deviation given by our
model outputs; and LMAs values were randomly sampled from a lognormal
distribution with the log-scale mean given by our model outputs and the
log-scale standard deviation raging from log(1.01) to log(10) across
simulated datasets. In contrast to GLOPNET and Panama sun leaves, our
analysis revealed a negative correlation of \emph{r} = -0.47 between
LMAm and LMAs for Panama shade leaves (see Appendix S5). Therefore, we
used a multivariate normal distribution with \emph{r} = -0.47 to
generate log(LMAm) and log(LMAs) for the Panama shade simulations.
Otherwise, the simulation protocol was as described above for GLOPNET
and Panama sun leaves.

For each simulated set of LMAm and LMAs values, we generated
\emph{A}\textsubscript{area} values using the estimated values of
\(\alpha_m\) and \(\alpha_s\) (Eq. 2) from the corresponding
best-fitting model (GLOPNET or Panama); i.e., the model with the lowest
LOOIC. We generated 1000 simulated datasets for GLOPNET, Panama sun
leaves, and Panama shade leaves, with each dataset having a sample size
of 100 leaves. For each simulated dataset, we used Eq. 5 to quantify the
relationship between \emph{A}\textsubscript{area} and LMA.

\hypertarget{appendix-s5-simulated-datasets-to-study-relationships-between-photosynthetic-capacity-and-lma}{%
\section{Appendix S5: Simulated datasets to study relationships between
photosynthetic capacity and
LMA}\label{appendix-s5-simulated-datasets-to-study-relationships-between-photosynthetic-capacity-and-lma}}

To create simulated datasets to study relationships between
photosynthetic capacity and LMA , we generated log-normally distributed
LMAm and LMAs values by sampling ln(LMAm) and ln(LMAs) from normal
distributions.

For GLOPNET and Panama sun leaves, results from the LMAm-LMAs models
(Eqs. 2- 4) indicated little correlation between LMAm and LMAs, so we
used univariate normal distributions (N) to generate the LMAm and LMAs
samples:

\[
\mathrm{ln}(\mathrm{LMAm}) \sim \mathrm{N}(\mathrm{ln}(\mu_m), \sigma_m) \qquad(\mathrm{S}5.1)
\]

\[
\mathrm{ln}(\mathrm{LMAs}) \sim \mathrm{N}(\mathrm{ln}(\mu_s), \sigma_s) \qquad(\mathrm{S}5.2)
\]

where \(\mathrm{ln}(\mu_m)\) and \(\mathrm{ln}(\mu_s)\) are the means of
LMAm and LMAs, respectively, on the log-scale and \(\sigma_m\) and
\(\sigma_s\) are the corresponding standard deviations. The parameters
\(\mathrm{ln}(\mu_m)\), \(\mathrm{ln}(\mu_s)\), \(\sigma_m\) and
\(\sigma_s\) were all estimated from the posterior medians of LMAm and
LMAs.

For Panama shade leaves, LMAm-LMAs model results indicated a negative
correlation between LMAm and LMAs, so we used a multivariate normal
distribution (MVN) to generate the LMAm and LMAs samples:

\[
\begin{bmatrix}
\mathrm{ln}(\mathrm{LMAm})\\
\mathrm{ln}(\mathrm{LMAs})
\end{bmatrix}
\sim \mathrm{MVN}
\left[
\begin{matrix}
\ln(\mu_{m}) &\\
\ln(\mu_{s}) &
\end{matrix},
\boldsymbol{\Sigma}
\right] \qquad(\mathrm{S}5.3)
\]

\[
\boldsymbol{\Sigma} = \
\begin{bmatrix}
\sigma_m^2 & \rho \sigma_m \sigma_s \\
\rho \sigma_m \sigma_s & \sigma_s^2 \\
\end{bmatrix} \qquad(\mathrm{S}5.4)
\]

where \(\boldsymbol{\Sigma}\) is the covariance matrix of ln(LMAm) and
ln(LMAs), and \(\rho\) is the correlation coefficient between ln(LMAm)
and ln(LMAs). The parameters in \(\boldsymbol{\Sigma}\) were all
estimated from the posterior medians of LMAm and LMAs.

To create simulated datasets (each with a sample size of 100) in which
LMAs accounted for different fractions of the total LMA variance, we
used the predictions (posterior medians) of LMAm and LMAs for each leaf
sample from the best models (Table 2) to estimate \(\mu_m\), \(\mu_s\),
\(\sigma_m\), and \(\rho\); and we varied \(\sigma_s\) from ln(1.01) to
ln(10) across simulated datasets. For each of the 100 leaves in a given
simulated dataset, \emph{A}\textsubscript{area} was calculated according
to Eq. 2.

Parameter values were \(\mu_m\) = 56.5, \(\mu_s\) = 61.2, \(\sigma_m\) =
0.83, \(\alpha_0\) = 1.77, \(\alpha_m\) = 0.28, and \(\alpha_s\) = -0.13
for GLOPNET; \(\mu_m\) = 47.3, \(\mu_s\) = 30.1, \(\sigma_m\) = 1.58,
\(\alpha_0\) = 0.34, \(\alpha_m\) = 0.56, and \(\alpha_s\) = 0 for
Panama sun leaves; and \(\mu_m\) = 7.6, \(\mu_s\) = 26.7, \(\sigma_m\) =
1.95, \(\alpha_0\) = 0.34, \(\alpha_m\) = 0.56, \(\alpha_s\) = 0, and
\(\rho\) = -0.47 for Panama shade leaves.

For each simulated dataset, we quantified the mass-dependence (\emph{b})
using the ordinary least squares regression (log-log) form of Eq. 5. We
repeated these steps 1000 times for each set of parameter values.

We also used Eqs. S5.1-S5.2 to create the simulated LMA datasets in Fig.
1, which is based on our analysis of GLOPNET data.
\emph{A}\textsubscript{area} values were generated using Eq. 2 and Eq.
S1. Based on our GLOPNET results, the standard deviations of ln(LMAs)
and ln(\emph{A}\textsubscript{area}) were set to 1 and 0.31,
respectively, and other GLOPNET parameter values are listed above.

\hypertarget{appendix-s6-variance-partitioning}{%
\section{Appendix S6: Variance
partitioning}\label{appendix-s6-variance-partitioning}}

To estimate the contributions of LMAm and LMAs to total LMA variance
(where LMA = LMAm + LMAs), we used the following identity:

\[
\mathrm{Var}(Y = X1 + X2) = \mathrm{Cov}(Y, X1+X2) = \mathrm{Cov}(Y,X1) + \mathrm{Cov}(Y,X2) \qquad(\mathrm{S}6)
\]

where Var(\(\cdot\)) is variance and Cov(\(\cdot\)) is covariance. Thus,
the fractions of total LMA variance due to variance in LMAm and LMAs are
Cov(LMA, LMAm)/Var(LMA) and Cov(LMA, LMAs)/Var(LMA), respectively. We
applied this method to the simulated datasets described above and also
to estimates of LMAm and LMAs for GLOPNET and Panama sun and shade
leaves. Note that Cov(LMA, LMAs)/Var(LMA) can be greater than 100\% when
there is a negative covariance between LMA and LMAm.

To estimate the variation in each LMA component between and within leaf
habits (evergreen vs.~deciduous), sites (wet vs.~dry), and light (sun
vs.~shade), we used ANOVA. Those post-hoc analyes were performed with
posterior median parameter values.

\newpage

\hypertarget{figures}{%
\section{Figures}\label{figures}}

\hypertarget{fig.-s1}{%
\subsection{Fig. S1}\label{fig.-s1}}

\includegraphics{/home/mattocci/LMAms/figs/box_inter.png}

\textbf{Fig. S1}: Boxplots comparing leaf mass per area (LMA),
photosynthetic leaf mass per area (LMAm; posterior medians), and
structural leaf mass per area (LMAs; posterior medians) across (a)
deciduous (Dev) and evergreen (Eve) leaves and (b) sites (wet and dry)
and canopy strata (sun and shade) in Panama. The results shown here
include all leaves in the Panama dataset, whereas Figs. 6- 7 in the main
text only include Panama species for which both sun and shade leaves
were available. Boxplot symbols as in Figs. 6- 7. Groups sharing the
same letters are not significantly different (P \textgreater{} 0.05;
t-tests). Estiamtes are from the best Panama model (Table 1).

\newpage

\hypertarget{fig.-s2}{%
\subsection{Fig. S2}\label{fig.-s2}}

\includegraphics{/home/mattocci/LMAms/figs/box_frac_de.png}

\textbf{Fig. S2}: Boxplots comparing posterior medians of the latent
variable \emph{f} (the fraction of total LMA comprised by LMAm) across
deciduous (Dev) and evergreen (Eve) leaves. Left: GLOPNET dataset.
Middle: All leaf samples in the Panama dataset. Right: Paired leaf
samples in the Panama dataset (species for which both sun and shade
leaves were available). Note that LMAm = \emph{f} \(\times\) LMA, and
LMAs = (1 -- \emph{f}) \(\times\) LMA. Boxplot symbols as in Fig. 6.
Groups sharing the same letters are not significantly different (P
\textgreater{} 0.05; t-tests). Estiamtes are from the best GLOPNET and
Panama models (Table 1).

\newpage

\hypertarget{fig.-s3}{%
\subsection{Fig. S3}\label{fig.-s3}}

\includegraphics{/home/mattocci/LMAms/figs/box_frac_pa.png}

\textbf{Fig. S3}: Boxplots comparing posterior medians of the latent
variable \emph{f} (the fraction of total LMA comprised by LMAm) across
sites (wet and dry) and canopy strata (sun and shade) in Panama. Left:
All leaf samples in the Panama dataset. Right: Paired leaf samples in
the Panama dataset (species for which both sun and shade leaves were
available). Note that LMAm = \emph{f} \(\times\) LMA, and LMAs = (1 --
\emph{f}) \(\times\) LMA. Boxplot symbols as in Fig. 7. Groups sharing
the same letters are not significantly different (P \textgreater{} 0.05;
t-tests). Estiamtes are from the best GLOPNET and Panama models (Table
1).

\newpage

\hypertarget{fig.-s4}{%
\subsection{Fig. S4}\label{fig.-s4}}

\includegraphics{/home/mattocci/LMAms/figs/mass_prop_sim.png}

\textbf{Fig. S4}: The relationships between mass dependency of
\emph{A}\textsubscript{max} (\emph{b} in Eq. 5 in the main text) and
LMAs variance (relative to total LMA variance; Eq. 6) for the different
values of the scaling exponents \(\alpha_m\) and \(\alpha_s\) (Eq. 2).
(a) The scaling exponent \(\alpha_m\) varies from 0.1 to 1.0 while the
scaling exponent \(\alpha_s\) is constant (\(\alpha_s\) = -0.13). (b)
The scaling exponent \(\alpha_s\) vary from -0.5 to 0.5 while the
scaling exponent \(\alpha_m\) is constant (\(\alpha_m\) = 0.28). Solid
lines indicate simulated medians and shaded regions indicate 95\%
confidence intervals. Empirical estimates of \emph{b} are typically
between 0 and 1 (\protect\hyperlink{ref-Osnas2018}{Osnas et al., 2018}).
\emph{A}\textsubscript{max} is primarily mass-dependent if \emph{b}
\textgreater{} 0.5, and primarily area-dependent if 0.5 \textgreater{}
\emph{b} \textgreater{} 0 (\protect\hyperlink{ref-Osnas2018}{Osnas et
al., 2018}). Parameter values are based on the best GLOPNET model (Table
1) and Appendix S5: \(\alpha_0\) = 1.77, \(\mu_m\) = 56.5, \(\mu_s\) =
61.2, and \(\sigma_m\) = 0.83.

\newpage

\hypertarget{fig.-s5}{%
\subsection{Fig. S5}\label{fig.-s5}}

\includegraphics{/home/mattocci/LMAms/figs/mass_prop_comp.png}

\textbf{Fig. S5}: The relationships between mass dependency of
\emph{A}\textsubscript{max} (\emph{b} in Eq. 5 in the main text) and
LMAs variance (relative to total LMA variance; Eq. 6) for simulated
datasets generated from a normal distribution (N) vs.~a multivariate
normal distribution (MVN). Parameter values are based on the best Panama
model (Table 1) and Appendix S5: \(\alpha_0\) = 0.34, \(\alpha_m\) =
0.56, \(\alpha_s\) = 0, \(\mu_m\) = 7.6, \(\mu_s\) = 26.7, \(\sigma_m\)
= 1.95, and \(\rho\) = -0.47.

\newpage

\hypertarget{fig.-s6}{%
\subsection{Fig. S6}\label{fig.-s6}}

\includegraphics{/home/mattocci/LMAms/figs/gl_point_np2.png}

\textbf{Fig. S6}: Measured traits related to photosynthesis and
metabolism (nitrogen and phosphorus per-unit leaf area;
\emph{N}\textsubscript{area} and \emph{P}\textsubscript{area}) are
positively correlated with LMA and with estimates (posterior medians) of
the metabolic and structural LMA components (LMAm and LMAs,
respectively) in the GLOPNET dataset. LMAm yields more consistent
relationships compared to LMA and LMAs; e.g., evergreen and deciduous
leaves align along a single relationship in panel b, but not in panels a
or c. Pearson correlation coefficients (\emph{r}) for LMA (left column)
and posterior medians of Pearson correlation coefficients (\(\bar{r}\))
for LMAm (middle column) and LMAs (right column) are shown.

\newpage

\hypertarget{fig.-s7}{%
\subsection{Fig. S7}\label{fig.-s7}}

\includegraphics{/home/mattocci/LMAms/figs/pa_point_npc_par.png}

\textbf{Fig. S7}: Partial regression plots for nitrogen, phosphorus and
cellulose per-unit leaf area (\emph{N}\textsubscript{area},
\emph{P}\textsubscript{area} and CL\textsubscript{area}). (a) LMAs
variation is controlled. (b) LMAm variation is controlled. The partial
regression plots show separation between sun and shade when controlling
for LMAs variation (i.e., LMAs does not explain the sun/shade
difference), but overlapping distributions of sun and shade when
controlling for LMAm variation (i.e., LMAm does explain the sun/shade
difference). Posterior medians of partial correlation coefficients
(\(\bar{\rho}\)) are shown.

\newpage

\hypertarget{fig.-s8}{%
\subsection{Fig. S8}\label{fig.-s8}}

\includegraphics{/home/mattocci/LMAms/figs/ps_point.png}

\textbf{Fig. S8} Pearson correlation coefficients for posterior medians
of LMAm vs LMAs in the (a) GLOPNET and (b) Panama datasets. The
non-significant or weak \emph{r} values indicate that a single axis
could not accurately represent the two-dimensional space. Symbols as in
main text Figs. 2- 3.

\newpage

\hypertarget{appendix-s7-stan-code}{%
\section{Appendix S7: Stan code}\label{appendix-s7-stan-code}}

\hypertarget{stan-code-for-the-glopnet-dataset}{%
\subsection{Stan code for the GLOPNET
dataset}\label{stan-code-for-the-glopnet-dataset}}

The best model for the GLOPNET dataset is LMAm-LMAs with the constraint
of \(\beta_m\) = 0.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{//}\AlertTok{NOTE}\CommentTok{: THIS STAN CODE IS GENERATED VIA "update.py"}
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} N;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] LMA;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] A;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] R;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] LL;}
\NormalTok{\}}

\KeywordTok{transformed data}\NormalTok{ \{}
  \DataTypeTok{vector}\NormalTok{[N] log\_A;}
  \DataTypeTok{vector}\NormalTok{[N] log\_LL;}
  \DataTypeTok{vector}\NormalTok{[N] log\_R;}
  \DataTypeTok{matrix}\NormalTok{[N, }\DecValTok{3}\NormalTok{] obs;}
  \DataTypeTok{vector}\NormalTok{[N] intercept;}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N)}
\NormalTok{    intercept[n] = }\DecValTok{1}\NormalTok{;}
\NormalTok{  log\_A = log(A);}
\NormalTok{  log\_LL = log(LL);}
\NormalTok{  log\_R = log(R);}
  \CommentTok{// use net photosynthesis (A) instead of gross (A + R)}
\NormalTok{  obs = append\_col(append\_col(log\_A, log\_LL), log\_R);}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \DataTypeTok{real}\NormalTok{ a0;}
  \DataTypeTok{real}\NormalTok{ am;}
  \DataTypeTok{real}\NormalTok{ as;}
  \DataTypeTok{real}\NormalTok{ b0;}
  \DataTypeTok{real}\NormalTok{ bs;}
  \DataTypeTok{real}\NormalTok{ g0;}
  \DataTypeTok{real}\NormalTok{ gm;}
  \DataTypeTok{real}\NormalTok{ gs;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{}[N] p;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[}\DecValTok{3}\NormalTok{] L\_sigma;}
  \DataTypeTok{cholesky\_factor\_corr}\NormalTok{[}\DecValTok{3}\NormalTok{] L\_Omega;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \DataTypeTok{matrix}\NormalTok{[N, }\DecValTok{3}\NormalTok{] Mu;}
  \DataTypeTok{matrix}\NormalTok{[}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{] Z;}
  \DataTypeTok{matrix}\NormalTok{[N, }\DecValTok{3}\NormalTok{] X;}
\NormalTok{  Z[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{] = a0;}
\NormalTok{  Z[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{] = b0;}
\NormalTok{  Z[}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{] = g0;}
\NormalTok{  Z[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{] = am;}
\NormalTok{  Z[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{] = }\DecValTok{0}\NormalTok{;}
\NormalTok{  Z[}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{] = gm;}
\NormalTok{  Z[}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{] = as;}
\NormalTok{  Z[}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{] = bs;}
\NormalTok{  Z[}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{] = gs;}

  \CommentTok{//log\_LMAm = log(LMA) + log(p);}
  \CommentTok{//log\_LMAs = log(LMA) + log(1 {-} p);}
  \CommentTok{//X = append\_col(append\_col(append\_col(intercept, log\_LMAm), log\_LMAs), leaf);}
\NormalTok{  X = append\_col(append\_col(intercept, log(LMA) + log(p)), log(LMA) + log(}\DecValTok{1}\NormalTok{ {-} p));}
\NormalTok{  Mu = X * Z;}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// priors}
\NormalTok{  a0 \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  b0 \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  g0 \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  am \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  bs \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  gm \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  gs \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  as \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  p \textasciitilde{} beta(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
\NormalTok{  L\_Omega \textasciitilde{} lkj\_corr\_cholesky(}\DecValTok{2}\NormalTok{);}
\NormalTok{  L\_sigma \textasciitilde{} cauchy(}\DecValTok{0}\NormalTok{, }\FloatTok{2.5}\NormalTok{);}

  \CommentTok{// model}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N)}
     \KeywordTok{target +=}\NormalTok{ multi\_normal\_cholesky\_lpdf(obs[i,] | Mu[i,], diag\_pre\_multiply(L\_sigma, L\_Omega));}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{vector}\NormalTok{[N] log\_lik;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{={-}}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} rho12;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{={-}}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} rho23;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{={-}}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} rho13;}
  \DataTypeTok{cov\_matrix}\NormalTok{[}\DecValTok{3}\NormalTok{] Sigma;}
  \DataTypeTok{vector}\NormalTok{[N] log\_LMAm;}
  \DataTypeTok{vector}\NormalTok{[N] log\_LMAs;}
\NormalTok{  log\_LMAm = log(LMA) + log(p);}
\NormalTok{  log\_LMAs = log(LMA) + log(}\DecValTok{1}\NormalTok{ {-} p);}
\NormalTok{  Sigma = diag\_pre\_multiply(L\_sigma, L\_Omega)}
\NormalTok{     * diag\_post\_multiply(L\_Omega\textquotesingle{}, L\_sigma);}
\NormalTok{  rho12 = Sigma[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{] * inv(L\_sigma[}\DecValTok{1}\NormalTok{] * L\_sigma[}\DecValTok{2}\NormalTok{]);}
\NormalTok{  rho23 = Sigma[}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{] * inv(L\_sigma[}\DecValTok{2}\NormalTok{] * L\_sigma[}\DecValTok{3}\NormalTok{]);}
\NormalTok{  rho13 = Sigma[}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{] * inv(L\_sigma[}\DecValTok{1}\NormalTok{] * L\_sigma[}\DecValTok{3}\NormalTok{]);}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N)}
\NormalTok{   log\_lik[i] = multi\_normal\_cholesky\_lpdf(obs[i,] | Mu[i,], diag\_pre\_multiply(L\_sigma, L\_Omega));}
\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

\hypertarget{stan-code-for-the-panama-dataset}{%
\subsection{Stan code for the Panama
dataset}\label{stan-code-for-the-panama-dataset}}

The best model for the Panama dataset is LMAm-LMAs-light with the
constraint of \(\alpha_s\) = 0 and \(\beta_m\) = 0.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{//}\AlertTok{NOTE}\CommentTok{: THIS STAN CODE IS GENERATED VIA "update.py"}
\KeywordTok{data}\NormalTok{ \{}
  \DataTypeTok{int}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{} N;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] LMA;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] A;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] R;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] LL;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[N] leaf;}
\NormalTok{\}}

\KeywordTok{transformed data}\NormalTok{ \{}
  \DataTypeTok{vector}\NormalTok{[N] log\_A;}
  \DataTypeTok{vector}\NormalTok{[N] log\_LL;}
  \DataTypeTok{vector}\NormalTok{[N] log\_R;}
  \DataTypeTok{matrix}\NormalTok{[N, }\DecValTok{3}\NormalTok{] obs;}
  \DataTypeTok{vector}\NormalTok{[N] intercept;}
  \ControlFlowTok{for}\NormalTok{ (n }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N)}
\NormalTok{    intercept[n] = }\DecValTok{1}\NormalTok{;}
\NormalTok{  log\_A = log(A);}
\NormalTok{  log\_LL = log(LL);}
\NormalTok{  log\_R = log(R);}
  \CommentTok{// use net photosynthesis (A) instead of gross (A + R)}
\NormalTok{  obs = append\_col(append\_col(log\_A, log\_LL), log\_R);}
\NormalTok{\}}

\KeywordTok{parameters}\NormalTok{ \{}
  \DataTypeTok{real}\NormalTok{ a0;}
  \DataTypeTok{real}\NormalTok{ am;}
  \DataTypeTok{real}\NormalTok{ b0;}
  \DataTypeTok{real}\NormalTok{ bs;}
  \DataTypeTok{real}\NormalTok{ g0;}
  \DataTypeTok{real}\NormalTok{ gm;}
  \DataTypeTok{real}\NormalTok{ gs;}
  \DataTypeTok{real}\NormalTok{ theta;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{}[N] p;}
  \DataTypeTok{vector}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{=}\DecValTok{0}\NormalTok{\textgreater{}[}\DecValTok{3}\NormalTok{] L\_sigma;}
  \DataTypeTok{cholesky\_factor\_corr}\NormalTok{[}\DecValTok{3}\NormalTok{] L\_Omega;}
\NormalTok{\}}

\KeywordTok{transformed parameters}\NormalTok{ \{}
  \DataTypeTok{matrix}\NormalTok{[N, }\DecValTok{3}\NormalTok{] Mu;}
  \DataTypeTok{matrix}\NormalTok{[}\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{] Z;}
  \DataTypeTok{matrix}\NormalTok{[N, }\DecValTok{4}\NormalTok{] X;}
\NormalTok{  Z[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{] = a0;}
\NormalTok{  Z[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{] = b0;}
\NormalTok{  Z[}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{] = g0;}
\NormalTok{  Z[}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{] = am;}
\NormalTok{  Z[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{] = }\DecValTok{0}\NormalTok{;}
\NormalTok{  Z[}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{] = gm;}
\NormalTok{  Z[}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{] = }\DecValTok{0}\NormalTok{;}
\NormalTok{  Z[}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{] = bs;}
\NormalTok{  Z[}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{] = gs;}
\NormalTok{  Z[}\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{] = }\DecValTok{0}\NormalTok{;}
\NormalTok{  Z[}\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{] = theta;}
\NormalTok{  Z[}\DecValTok{4}\NormalTok{, }\DecValTok{3}\NormalTok{] = }\DecValTok{0}\NormalTok{;}

  \CommentTok{//log\_LMAm = log(LMA) + log(p);}
  \CommentTok{//log\_LMAs = log(LMA) + log(1 {-} p);}
  \CommentTok{//X = append\_col(append\_col(append\_col(intercept, log\_LMAm), log\_LMAs), leaf);}
\NormalTok{  X = append\_col(append\_col(append\_col(intercept,}
\NormalTok{    log(LMA) + log(p)),}
\NormalTok{    log(LMA) + log(}\DecValTok{1}\NormalTok{ {-} p)),}
\NormalTok{    leaf);}
\NormalTok{  Mu = X * Z;}
\NormalTok{\}}

\KeywordTok{model}\NormalTok{ \{}
  \CommentTok{// priors}
\NormalTok{  a0 \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  b0 \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  g0 \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  am \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  bs \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  gm \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  gs \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  theta \textasciitilde{} normal(}\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{);}
\NormalTok{  p \textasciitilde{} beta(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{);}
\NormalTok{  L\_Omega \textasciitilde{} lkj\_corr\_cholesky(}\DecValTok{2}\NormalTok{);}
\NormalTok{  L\_sigma \textasciitilde{} cauchy(}\DecValTok{0}\NormalTok{, }\FloatTok{2.5}\NormalTok{);}

  \CommentTok{// model}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N)}
     \KeywordTok{target +=}\NormalTok{ multi\_normal\_cholesky\_lpdf(obs[i,] | Mu[i,], diag\_pre\_multiply(L\_sigma, L\_Omega));}
\NormalTok{\}}

\KeywordTok{generated quantities}\NormalTok{ \{}
  \DataTypeTok{vector}\NormalTok{[N] log\_lik;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{={-}}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} rho12;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{={-}}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} rho23;}
  \DataTypeTok{real}\NormalTok{\textless{}}\KeywordTok{lower}\NormalTok{={-}}\DecValTok{1}\NormalTok{, }\KeywordTok{upper}\NormalTok{=}\DecValTok{1}\NormalTok{\textgreater{} rho13;}
  \DataTypeTok{cov\_matrix}\NormalTok{[}\DecValTok{3}\NormalTok{] Sigma;}
  \DataTypeTok{vector}\NormalTok{[N] log\_LMAm;}
  \DataTypeTok{vector}\NormalTok{[N] log\_LMAs;}
\NormalTok{  log\_LMAm = log(LMA) + log(p);}
\NormalTok{  log\_LMAs = log(LMA) + log(}\DecValTok{1}\NormalTok{ {-} p);}
\NormalTok{  Sigma = diag\_pre\_multiply(L\_sigma, L\_Omega)}
\NormalTok{     * diag\_post\_multiply(L\_Omega\textquotesingle{}, L\_sigma);}
\NormalTok{  rho12 = Sigma[}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{] * inv(L\_sigma[}\DecValTok{1}\NormalTok{] * L\_sigma[}\DecValTok{2}\NormalTok{]);}
\NormalTok{  rho23 = Sigma[}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{] * inv(L\_sigma[}\DecValTok{2}\NormalTok{] * L\_sigma[}\DecValTok{3}\NormalTok{]);}
\NormalTok{  rho13 = Sigma[}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{] * inv(L\_sigma[}\DecValTok{1}\NormalTok{] * L\_sigma[}\DecValTok{3}\NormalTok{]);}
  \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\NormalTok{:N)}
\NormalTok{   log\_lik[i] = multi\_normal\_cholesky\_lpdf(obs[i,] | Mu[i,], diag\_pre\_multiply(L\_sigma, L\_Omega));}
\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-Alvarez2014}{}}%
Alvarez, I., Niemi, J., Simpson, M., 2014. Bayesian inference for a
covariance matrix. \url{https://doi.org/10.1214/aos/1176348885}

\leavevmode\vadjust pre{\hypertarget{ref-Betancourt2016}{}}%
Betancourt, M., 2016. Diagnosing {Suboptimal Cotangent Disintegrations}
in {Hamiltonian Monte Carlo}. arXiv.
\url{https://doi.org/10.48550/arXiv.1604.00695}

\leavevmode\vadjust pre{\hypertarget{ref-Blonder2011}{}}%
Blonder, B., Violle, C., Bentley, L.P., Enquist, B.J., 2011. Venation
networks and the origin of the leaf economics spectrum. Ecology Letters
14, 91--100. \url{https://doi.org/10.1111/j.1461-0248.2010.01554.x}

\leavevmode\vadjust pre{\hypertarget{ref-Gelman2008}{}}%
Gelman, A., Jakulin, A., Pittau, M.G., Su, Y.S., 2008. A weakly
informative default prior distribution for logistic and other regression
models. Annals of Applied Statistics 2, 1360--1383.
\url{https://doi.org/10.1214/08-AOAS191}

\leavevmode\vadjust pre{\hypertarget{ref-Lemoine2019}{}}%
Lemoine, N.P., 2019. Moving beyond noninformative priors: Why and how to
choose weakly informative priors in {Bayesian} analyses. Oikos 128,
912--928. \url{https://doi.org/10.1111/oik.05985}

\leavevmode\vadjust pre{\hypertarget{ref-Lewandowski2009}{}}%
Lewandowski, D., Kurowicka, D., Joe, H., 2009. Generating random
correlation matrices based on vines and extended onion method. Journal
of Multivariate Analysis 100, 1989--2001.
\url{https://doi.org/10.1016/j.jmva.2009.04.008}

\leavevmode\vadjust pre{\hypertarget{ref-Lloyd2013}{}}%
Lloyd, J., Bloomfield, K., Domingues, T.F., Farquhar, G.D., 2013.
Photosynthetically relevant foliar traits correlating better on a mass
vs an area basis: {Of} ecophysiological relevance or just a case of
mathematical imperatives and statistical quicksand? New Phytologist 199,
311--321. \url{https://doi.org/10.1111/nph.12281}

\leavevmode\vadjust pre{\hypertarget{ref-Osnas2018}{}}%
Osnas, J.L.D., Katabuchi, M., Kitajima, K., Wright, S.J., Reich, P.B.,
Van Bael, S.A., Kraft, N.J.B., Samaniego, M.J., Pacala, S.W., Lichstein,
J.W., 2018. Divergent drivers of leaf trait variation within species,
among species, and among functional groups. Proceedings of the National
Academy of Sciences of the United States of America 115, 5480--5485.
\url{https://doi.org/10.1073/pnas.1803989115}

\leavevmode\vadjust pre{\hypertarget{ref-Osnas2013}{}}%
Osnas, J.L.D., Lichstein, J.W., Reich, P.B., Pacala, S.W., 2013. Global
leaf trait relationships: {Mass}, area, and the leaf economics spectrum.
Science 340, 741--744. \url{https://doi.org/10.1126/science.1231574}

\leavevmode\vadjust pre{\hypertarget{ref-Shipley2006}{}}%
Shipley, B., Lechowicz, M.J., Wright, I., Reich, P.B., 2006. Fundamental
trade-offs generating the worldwide leaf economics spectrum. Ecology 87,
535--541. \url{https://doi.org/10.1890/05-1051}

\leavevmode\vadjust pre{\hypertarget{ref-Westoby2013}{}}%
Westoby, M., Reich, P.B., Wright, I.J., 2013. Understanding ecological
variation across species: {Area-based} vs mass-based expression of leaf
traits. New Phytologist 199, 322--323.
\url{https://doi.org/10.1111/nph.12345}

\leavevmode\vadjust pre{\hypertarget{ref-Wright2004a}{}}%
Wright, I.J., Reich, P.B., Westoby, M., Ackerly, D.D., Baruch, Z.,
Bongers, F., Cavender-Bares, J., Chapin, T., Cornellssen, J.H.C.,
Diemer, M., Flexas, J., Garnier, E., Groom, P.K., Gulias, J., Hikosaka,
K., Lamont, B.B., Lee, T., Lee, W., Lusk, C., Midgley, J.J., Navas,
M.L., Niinemets, ., Oleksyn, J., Osada, H., Poorter, H., Pool, P.,
Prior, L., Pyankov, V.I., Roumet, C., Thomas, S.C., Tjoelker, M.G.,
Veneklaas, E.J., Villar, R., 2004. The worldwide leaf economics
spectrum. Nature 428, 821--827.
\url{https://doi.org/10.1038/nature02403}

\end{CSLReferences}



\end{document}
